# -*- coding: utf-8 -*-
"""Progetto_Nuovo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-AFFIy95oFyPQXPVTh6y2TywKbqGPjZK
"""

#Import delle librerie necessarie per il progetto
import pandas as pd #libreria per i dataframe
import networkx.algorithms as nxa 
import collections
import networkx as nx #Libreria che si occupa della gestione 
import matplotlib.pyplot as plt
import re 
import itertools
import statistics
import numpy as np

from wordcloud import WordCloud  #Libreria per la creazione di wordcloud
from collections import Counter
from textblob import TextBlob #libreria non più utilizzata in quanto non comprendeva l'italiano

#Libreria per interfacciarsi con di Twitter 
import tweepy 
from tweepy import OAuthHandler

#Libreria per eliminare le stopwords, con il downlaod della lingua ialiana e dell'aggiunta di nuove stopwords
import nltk
from nltk.corpus import stopwords
nltk.download("stopwords")
stop_words = set(stopwords.words("italian")) 
new_stopwords = ["pi", "più","sar","de"]
second_stopwrds = set(stopwords.words("english")) 
stop_words = stop_words.union(new_stopwords)
stop_words = stop_words.union(second_stopwrds)
from nltk import bigrams

#Funzione che ci consente di eliminare dai tweets dei caratteri definiti e sostituirli con uno spazio
def remove_url(txt):    
    return " ".join(re.sub("([^0-9A-Za-z \t])|(\w+:\/\/\S+)", "", txt).split())

#installazione della libreria che si occupa della sentiment e della emotion analysis
!pip install feel_it
from feel_it import EmotionClassifier, SentimentClassifier

#Dati necessari per l'uso delle api di Twitter
consumer_key = 'letueapi'
consumer_secret = 'letueapi'
access_token = '3letueapi'
access_secret = 'letueapi'
 
auth = OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_secret)
 
api = tweepy.API(auth, wait_on_rate_limit=True)

#funzione che data una tabella cntenente i tweets in cui è menzionato un dato politico esegue la percentuale del numero di tweets con sentimenti positivi-negativi & con le 4 emozioni prese in considerazione
#stampando il tutto su un grafico a torta
def sentiment_emotion(table,name):
  app_1=table[table.Emotion=="fear"]
  app_1=app_1["Testo"]
  fear=round((app_1.shape[0]/table.shape[0])*100,1)
  app_2=table[table.Emotion=="joy"]
  app_2=app_2["Testo"]
  joy=round((app_2.shape[0]/table.shape[0])*100,1)
  app_3=table[table.Emotion=="anger"]
  app_3=app_3["Testo"]
  anger=round((app_3.shape[0]/table.shape[0])*100,1)
  app_4=table[table.Emotion=="sadness"]
  app_4=app_4["Testo"]
  sadness=round((app_4.shape[0]/table.shape[0])*100,1)
  app_5=table[table.Sentiment=="positive"]
  app_5=app_5["Testo"]
  positive=round((app_5.shape[0]/table.shape[0])*100,1)
  app_7=table[table.Sentiment=="negative"]
  app_7=app_7["Testo"]
  negative=round((app_7.shape[0]/table.shape[0])*100,1)
  labels = ["Fear ["+str(fear)+"%]" , "Joy ["+str(joy)+"%]","Anger ["+str(anger)+"%]","Sadness ["+str(sadness)+"%]"]
  sizes = [fear, joy,anger, sadness]
  colors = ["green", "yellow","red","blue"]
  patches, texts = plt.pie(sizes,colors=colors, startangle=90)
  plt.style.use("default")
  plt.legend(labels)
  plt.title("Emotion recognition " +str(name)+ " in percentuale")
  plt.axis("equal")
  plt.show()
  labels = ["Positive ["+str(positive)+"%]" ,"Negative ["+str(negative)+"%]"] 
  sizes = [positive, negative] 
  colors = ["green","red"] 
  patches, texts = plt.pie(sizes,colors=colors, startangle=90)
  plt.style.use("default")
  plt.legend(labels)
  plt.title("Sentiment Analysis of "+str(name)+"'s tweets  in percentuale")
  plt.axis("equal")
  plt.show()

#funzione che data una tabella contenente i tweets in cui è menzionato un dato politico esegue il conteggio dei singoli tweets con sentimenti positivi-negativi & con le 4 emozioni prese in considerazione
#stampando il tutto su un grafico a barre orizzontali per poterle confrontare
def grafici_num(table,name):
  plt.figure(figsize=(8,6))
  table["Sentiment"].value_counts()
  table["Sentiment"].value_counts().plot(kind="barh")
  plt.title("Number of Positive and negative tweets of: " +str(name))
  plt.show()
  plt.figure(figsize=(8,6))
  table["Emotion"].value_counts()
  table["Emotion"].value_counts().plot(kind="barh")
  plt.title("Numero di tweets felici, tristi e impauriti riferiti a " +str(name))
  plt.show()

#Limite massimo di tweets da prendere per singolo esponente politico
num_max_tweets=1000

#Dichiarazioni delle liste che conterranno il contenuto dei tweet, la data e il numero di likes
testo=[]
nlike=[]
day=[]
rtc=[]
twn=[]

#Lista dei politici analizzati
list_polit=["GiorgiaMeloni","matteosalvinimi","luigidimaio","GiuseppeConteIT","EnricoLetta"]

#Dichiarazione di parte delle variabili utilizzate nel progetto
like_tot=[]
retweets_tot=[]
user_base=[]
perc_user_like=[]
perc_user_ret=[]
appoggio=[]
appoggio3=[]
like_activity=[]
re_activity=[]

tweet_meloni=[]
meloni_norm=[]
lista_meloni=[]
token_meloni=[]
contatore_parole_meloni=[]
appoggio_meloni=[]

tweet_salvini=[]
salvini_norm=[]
lista_salvini=[]
token_salvini=[]
contatore_parole_salvini=[]
appoggio_salvini=[]

tweet_dimaio=[]
dimaio_norm=[]
lista_dimaio=[]
token_dimaio=[]
contatore_parole_dimaio=[]
appoggio_dimaio=[]

tweet_conte=[]
conte_norm=[]
lista_conte=[]
token_conte=[]
contatore_parole_conte=[]
appoggio_conte=[]

tweet_letta=[]
letta_norm=[]
lista_letta=[]
token_letta=[]
contatore_parole_letta=[]
appoggio_letta=[]

char_letta=0
char_meloni=0
char_salvini=0
char_dimaio=0
char_conte=0
tot_nu=0

#Blocco per collezionare i dati da Twitter. Per ogni politico vengono scaricati X Tweets con relativo ID, testo, numero di likes, numero di Retweets e la data di post.
#Questi vengono poi collocati tramite l'append nelle singole variabili
for j in range(len(list_polit)):
  for i in tweepy.Cursor(api.user_timeline, id=list_polit[j], tweet_mode="extended").items(num_max_tweets):
    if "RT @" not in i.full_text: #Vengono presi i contenuti privi della scritta RT @, eliminando così i retweets
      testo.append(i.full_text)
      nlike.append(i.favorite_count)
      day.append(i.created_at)
      rtc.append(i.retweet_count)
      twn.append(i.user.screen_name)

#Blocco per la creazione di un dataframe (tabella) usando la libreria Panda
tabella=pd.DataFrame({"ID":twn,"Tweet":testo,"Likes":nlike,"Retweet":rtc,"Data":day})
tabella

#Usando le funzioni di Pandas si richiama a schermo una tabella con la media dei likes e dei retweet 
media_like_rt=tabella.groupby("ID").mean()
media_like_rt

#Usando le funzioni di Pandas si richiama a schermo una tabella con la mediana dei likes e dei retweet 
mediana_like_rt=tabella.groupby("ID").median()
mediana_like_rt

#Trasferimento dei dati presenti nella precedente tabella delle medie in variabili appoggio così da poter creare un grafico
tab_likes_letta = media_like_rt.values[0][0]
tab_likes_meloni = media_like_rt.values[1][0]
tab_likes_conte = media_like_rt.values[2][0]
tab_likes_maio = media_like_rt.values[3][0]
tab_likes_salvini =  media_like_rt.values[4][0]
tab_rt_letta = media_like_rt.values[0][1]
tab_rt_meloni = media_like_rt.values[1][1]
tab_rt_conte = media_like_rt.values[2][1]
tab_rt_maio = media_like_rt.values[3][1]
tab_rt_salvini =  media_like_rt.values[4][1]

#Trasferimento dei dati presenti nella precedente tabella della mediana in variabili appoggio così da poter creare un grafico
tab1_likes_letta = mediana_like_rt.values[0][0]
tab1_likes_meloni = mediana_like_rt.values[1][0]
tab1_likes_conte = mediana_like_rt.values[2][0]
tab1_likes_maio = mediana_like_rt.values[3][0]
tab1_likes_salvini =  mediana_like_rt.values[4][0]
tab1_rt_letta = mediana_like_rt.values[0][1]
tab1_rt_meloni = mediana_like_rt.values[1][1]
tab1_rt_conte = mediana_like_rt.values[2][1]
tab1_rt_maio = mediana_like_rt.values[3][1]
tab1_rt_salvini = mediana_like_rt.values[4][1]

#Istruzioni per la stampa dei grafici per confronto delle medie di Like e Retweets per un singolo tweet
f, ax = plt.subplots(figsize=(13,10))
croma=["blue","green","yellow","orange","red"] #Scelta del colore
x=["Meloni","Salvini","Di Maio","Conte","Letta"] #Posizionamento dei nomi sull'asse X
y=[tab_likes_meloni, tab_likes_salvini, tab_likes_maio, tab_likes_conte, tab_likes_letta] #Posizionamento dei valori sull'asse Y
plt.bar(x,y, width=0.7, color=croma)
plt.ylabel("Media di likes per tweet")
plt.title("Confronto delle medie di likes per singolo tweet")
plt.show()

f, ax = plt.subplots(figsize=(13,10))
croma=["blue","green","yellow","orange","red"]
x=["Meloni","Salvini","Di Maio","Conte","Letta"]
y=[tab_rt_meloni, tab_rt_salvini, tab_rt_maio, tab_rt_conte, tab_rt_letta]
plt.bar(x,y, width=0.7, color=croma)
plt.ylabel("Media di likes per retweets")
plt.title("Confronto delle medie di Retweets per singolo tweet")
plt.show()

#Grafici per confronto delle mediane di Like e Retweets per un singolo tweet
f, ax = plt.subplots(figsize=(13,10))
croma=["blue","green","yellow","orange","red"]
x=["Meloni","Salvini","Di Maio","Conte","Letta"]
y=[tab1_likes_meloni, tab1_likes_salvini, tab1_likes_maio, tab1_likes_conte, tab1_likes_letta]
plt.bar(x,y, width=0.7, color=croma)
plt.ylabel("Mediana di likes per tweet")
plt.title("Confronto delle mediana di likes per singolo tweet")
plt.show()

f, ax = plt.subplots(figsize=(13,10))
croma=["blue","green","yellow","orange","red"]
x=["Meloni","Salvini","Di Maio","Conte","Letta"]
y=[tab1_rt_meloni, tab1_rt_salvini, tab1_rt_maio, tab1_rt_conte, tab1_rt_letta]
plt.bar(x,y, width=0.7, color=croma)
plt.ylabel("Mediana di retweets per tweet")
plt.title("Confronto delle mediane di Retweets per singolo tweet")
plt.show()

#Blocco che calcola la media della lunghezza dei tweet

#Tramite il ciclo for viene fatta scorrere la variabile che contiene gli ID dei profili
#se viene trovata corrispondenza tra l'ID e il nome del politico viene aggiornato un contatore
#e una variabile in cui si aggiunge la lunghezza del testo sommata alla variabile stessa.
#A fine ciclo il contatore ci permette di sapere quanti post erano stati scritti da un singolo
#politico così da poter trarne la media.

for j in range(len(twn)):
  if twn[j]=="EnricoLetta":
    tot_nu=tot_nu+1
    char_letta=char_letta+len(testo[j])

media_char_letta=char_letta/tot_nu

tot_nu=0
for j in range(len(twn)):
  if twn[j]=="matteosalvinimi":
    tot_nu=tot_nu+1
    char_salvini=char_salvini+len(testo[j])

media_char_salvini=char_salvini/tot_nu

tot_nu=0
for j in range(len(twn)):
  if twn[j]=="GiorgiaMeloni":
    tot_nu=tot_nu+1
    char_meloni=char_meloni+len(testo[j])

media_char_meloni=char_meloni/tot_nu

tot_nu=0
for j in range(len(twn)):
  if twn[j]=="luigidimaio":
    tot_nu=tot_nu+1
    char_dimaio=char_dimaio+len(testo[j])

media_char_dimaio=char_dimaio/tot_nu

tot_nu=0
for j in range(len(twn)):
  if twn[j]=="GiuseppeConteIT":
    tot_nu=tot_nu+1
    char_conte=char_conte+len(testo[j])

media_char_conte=char_conte/tot_nu

#Stampa delle medie    
print("Meloni: "+str(media_char_meloni))
print("Salvini: "+str(media_char_salvini))
print("Di Maio: "+str(media_char_dimaio))
print("Conte: "+str(media_char_conte))
print("Letta: "+str(media_char_letta))

#Visualizzazione delle medie su un diagramma a barre
f, ax = plt.subplots(figsize=(13,10))
croma=["blue","green","yellow","orange","red"]
x=["Meloni","Salvini","Di Maio","Conte","Letta"]
y=[media_char_meloni, media_char_salvini, media_char_dimaio, media_char_conte, media_char_letta]
plt.bar(x,y, width=0.7, color=croma)
plt.ylabel("Media di caratteri per tweet")
plt.title("Confronto delle medie di caratteri utilizzati per singolo tweet")
plt.show()

#Tramite Pandas si richiama la top 10 dei tweets più likati
top10=tabella.loc[tabella.Likes.nlargest(10).index]
top10

#Tramite Pandas si richiama la bottom 10 dei tweets meno likati
bot10=tabella.loc[tabella.Likes.nsmallest(10).index]
bot10

#Codice per la raccolta e la tokenizzazione delle parole di ogni singolo esponente 
#al fine di poter calcolare quali sono le più utilizzate.

#L'array scorre l'intera variabile che contiene gli ID dei Tweets, entrando 
#nell'if quando viene trovato il nome desiderato. Qui, tramite la funzione
#append viene viene collocata in una variabile il testo del singolo tweet.
for i in range(len(twn)):
  if twn[i]=="GiorgiaMeloni":
    tweet_meloni.append(testo[i])

for i in range(len(twn)):
  if twn[i]=="matteosalvinimi":
    tweet_salvini.append(testo[i])

for i in range(len(twn)):
  if twn[i]=="luigidimaio":
    tweet_dimaio.append(testo[i])

for i in range(len(twn)):
  if twn[i]=="GiuseppeConteIT":
    tweet_conte.append(testo[i])

for i in range(len(twn)):
  if twn[i]=="EnricoLetta":
    tweet_letta.append(testo[i])

#La variabile precedentemente creata, contenente tutti i tweets del singolo
#esponente politico viene ora "normalizzata". Dal testo vengono rimossi
#caratteri speciali, richiamando la funzione dichiarata all'inizio ed eventuali
#link. Poi tutte le parole vengono rese in minuscolo per evitare ripetizioni.
#Inoltre, vengono eliminate le stopwords e poi contante le singole parole per 
#poi inserire in un dataframe le 10 parole più comuni.

meloni_norm=[remove_url(tweet_meloni) for tweet_meloni in tweet_meloni]  #Rimozione URL e caratteri speciali.
lista_meloni=[meloni_norm.lower().split() for meloni_norm in meloni_norm] #Messa in minuscolo.
for tutte_par in lista_meloni: #Climinazione delle stopwords.
    for tweet_meloni in tutte_par:
      token_meloni = [[parola for parola in lista_meloni if not parola in stop_words] 
              for lista_meloni in lista_meloni]
appoggio_meloni= list(itertools.chain(*token_meloni))  #Creazione di una lista delle parole.
contatore_parole_meloni = collections.Counter(appoggio_meloni) #Conteggio delle ripetizioni.
tabella_meloni_parole = pd.DataFrame(contatore_parole_meloni.most_common(10), columns=['Parole', 'Ripetizioni']) #Creazione della tabella.

salvini_norm=[remove_url(tweet_salvini) for tweet_salvini in tweet_salvini] 
lista_salvini=[salvini_norm.lower().split() for salvini_norm in salvini_norm] 
for tutte_par in lista_salvini:
    for tweet_salvini in tutte_par:
      token_salvini = [[parola for parola in lista_salvini if not parola in stop_words]
              for lista_salvini in lista_salvini]
appoggio_salvini = list(itertools.chain(*token_salvini))
contatore_parole_salvini = collections.Counter(appoggio_salvini) 
tabella_salvini_parole = pd.DataFrame(contatore_parole_salvini.most_common(10), columns=['Parole', 'Ripetizioni'])

dimaio_norm=[remove_url(tweet_dimaio) for tweet_dimaio in tweet_dimaio] 
lista_dimaio=[dimaio_norm.lower().split() for dimaio_norm in dimaio_norm] 
for tutte_par in lista_dimaio:
    for tweet_dimaio in tutte_par:
      token_dimaio = [[parola for parola in lista_dimaio if not parola in stop_words]
              for lista_dimaio in lista_dimaio]
appoggio_dimaio = list(itertools.chain(*token_dimaio)) 
contatore_parole_dimaio = collections.Counter(appoggio_dimaio) 
tabella_dimaio_parole = pd.DataFrame(contatore_parole_dimaio.most_common(10), columns=['Parole', 'Ripetizioni'])

conte_norm=[remove_url(tweet_conte) for tweet_conte in tweet_conte] 
lista_conte=[conte_norm.lower().split() for conte_norm in conte_norm] 
for tutte_par in lista_conte:
    for tweet_conte in tutte_par:
      token_conte = [[parola for parola in lista_conte if not parola in stop_words]
              for lista_conte in lista_conte]
appoggio_conte = list(itertools.chain(*token_conte))
contatore_parole_conte = collections.Counter(appoggio_conte) 
tabella_conte_parole = pd.DataFrame(contatore_parole_conte.most_common(10), columns=['Parole', 'Ripetizioni'])

letta_norm=[remove_url(tweet_letta) for tweet_letta in tweet_letta] 
lista_letta=[letta_norm.lower().split() for letta_norm in letta_norm] 
for tutte_par in lista_letta:
    for tweet_letta in tutte_par:
      token_letta = [[parola for parola in lista_letta if not parola in stop_words]
              for lista_letta in lista_letta]
appoggio_letta = list(itertools.chain(*token_letta)) 
contatore_parole_letta = collections.Counter(appoggio_letta) 
tabella_letta_parole = pd.DataFrame(contatore_parole_letta.most_common(10), columns=['Parole', 'Ripetizioni'])

#Grafici dei singoli esponenti con le dieci parole più utilizzate.

fig, ax = plt.subplots(figsize=(15, 10))
tabella_letta_parole.sort_values(by='Ripetizioni').plot.barh(x='Parole',y='Ripetizioni',ax=ax,color="red")
ax.set_title("Lista delle parole più comuni priva di stopwords: Enrico Letta")
plt.show()
fig, ax = plt.subplots(figsize=(15, 10))
tabella_dimaio_parole.sort_values(by='Ripetizioni').plot.barh(x='Parole',y='Ripetizioni',ax=ax,color="yellow")
ax.set_title("Lista delle parole più comuni priva di stopwords: Luigi Di Maio")
plt.show()
fig, ax = plt.subplots(figsize=(15, 10))
tabella_salvini_parole.sort_values(by='Ripetizioni').plot.barh(x='Parole',y='Ripetizioni',ax=ax,color="green")
ax.set_title("Lista delle parole più comuni priva di stopwords: Matteo Salvini")
plt.show()
fig, ax = plt.subplots(figsize=(15, 10))
tabella_meloni_parole.sort_values(by='Ripetizioni').plot.barh(x='Parole',y='Ripetizioni',ax=ax,color="blue")
ax.set_title("Lista delle parole più comuni priva di stopwords: Giorgia Meloni")
plt.show()
fig, ax = plt.subplots(figsize=(15, 10))
tabella_conte_parole.sort_values(by='Ripetizioni').plot.barh(x='Parole',y='Ripetizioni',ax=ax,color="orange")
ax.set_title("Lista delle parole più comuni priva di stopwords: Giuseppe Conte")
plt.show()

tabella_letta_parole #Tabella con la top 10 delle parole utilizzate da Letta

tabella_salvini_parole  #Tabella con la top 10 delle parole utilizzate da Letta

tabella_dimaio_parole #Tabella con la top 10 delle parole utilizzate da Letta

tabella_meloni_parole #Tabella con la top 10 delle parole utilizzate da Letta

tabella_conte_parole #Tabella con la top 10 delle parole utilizzate da Letta

#L'operazione prima eseguita per ogni singolo esponente politico viene qui invece
#fatta per la totalità dei Tweets così da vedere in maniera globale la top 10
#delle parole più utilizzate.

dati_norm=[remove_url(testo) for testo in testo]
lista_parole=[dati_norm.lower().split() for dati_norm in dati_norm] 
for tutte_par in lista_parole:
    for testo in tutte_par:
      parole_singole = [[parola for parola in lista_parole if not parola in stop_words]
              for lista_parole in lista_parole]
appoggio2 = list(itertools.chain(*parole_singole)) 
contatore_parole = collections.Counter(appoggio2) 
risultato_fin = pd.DataFrame(contatore_parole.most_common(10),columns=['Parole', 'Ripetizioni'])
risultato_fin

#Stampa del grafico con la lista delle 10 parole più comuni nella totalità dei tweets.

fig, ax = plt.subplots(figsize=(15, 10))
risultato_fin.sort_values(by='Ripetizioni').plot.barh(x='Parole',y='Ripetizioni',ax=ax,color="orange")
ax.set_title("Most common words without stopwords")
plt.show()

#Blocco di istruzioni per la creazione di un wordcloud tramite l'ausilio della#libreria WordCloud.

conteggio=Counter(contatore_parole) #Variabile contenente il conteggio della ripetizione delle parole, realizzata tramite l'ausilio della libreria collections
wordcloud = WordCloud(width = 1250, height = 725).generate_from_frequencies(conteggio) #creazione del wordcloud, assegnazione delle dimensioni e della variabile da cui trarre i dati
plt.figure(figsize=(15,8))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()
plt.close()

#il blocco sottostante si occupa di ricercare 1000 tweets che menzionano salvini
name_lista_salvini=[]
testo_mention_salvini=[]
parola_chiave_1 = '@matteosalvinimi' #assegnazione ad una variabile della mention da cercare
filtro='-filter:retweets' #variabile filtro che ci consente di escludere i retweets
tweets_sal = tweepy.Cursor(api.search, q=parola_chiave_1+filtro, lang="it").items(num_max_tweets)
for tweet in tweets_sal:
 testo_mention_salvini.append(tweet.text) #tramite il ciclo for si scorrono le mention e si assegna alla variabile l'id di chi posta il tweet e il contenuto del tweet
 name_lista_salvini.append(tweet.author.name)
mention_sal_tab=pd.DataFrame({"ID":name_lista_salvini, "Tweet":testo_mention_salvini}) # si crea quindi un dataframe con i dati ottenuti
#il blocco sottostante si occupa di ricercare 1000 tweets che menzionano meloni
name_lista_mel=[]
testo_mention_mel=[]
parola_chiave_2 = '@GiorgiaMeloni'
filtro='-filter:retweets'
tweets_mel = tweepy.Cursor(api.search, q=parola_chiave_2+filtro, lang="it").items(num_max_tweets)
for tweet in tweets_mel:
 testo_mention_mel.append(tweet.text)
 name_lista_mel.append(tweet.author.name)
mention_mel_tab=pd.DataFrame({"ID":name_lista_mel, "Tweet":testo_mention_mel})
#il blocco sottostante si occupa di ricercare 1000 tweets che menzionano letta
name_lista_let=[]
testo_mention_let=[]
parola_chiave_3 = '@EnricoLetta'
filtro='-filter:retweets'
tweets_let = tweepy.Cursor(api.search, q=parola_chiave_3+filtro, lang="it").items(num_max_tweets)
for tweet in tweets_let:
 testo_mention_let.append(tweet.text)
 name_lista_let.append(tweet.author.name)
mention_let_tab=pd.DataFrame({"ID":name_lista_let, "Tweet":testo_mention_let})
#il blocco sottostante si occupa di ricercare 1000 tweets che menzionano conte
name_lista_cont=[]
testo_mention_cont=[]
parola_chiave_4 = '@GiuseppeConteIT'
filtro='-filter:retweets'
tweets_cont = tweepy.Cursor(api.search, q=parola_chiave_4+filtro, lang="it").items(num_max_tweets)
for tweet in tweets_cont:
 testo_mention_cont.append(tweet.text)
 name_lista_cont.append(tweet.author.name)
mention_cont_tab=pd.DataFrame({"ID":name_lista_cont, "Tweet":testo_mention_cont})
#il blocco sottostante si occupa di ricercare 1000 tweets che menzionano dimaio
name_lista_maio=[]
testo_mention_maio=[]
parola_chiave_5 = '@luigidimaio'
filtro='-filter:retweets'
tweets_maio = tweepy.Cursor(api.search, q=parola_chiave_5+filtro, lang="it").items(num_max_tweets)
for tweet in tweets_maio:
 testo_mention_maio.append(tweet.text)
 name_lista_maio.append(tweet.author.name)
mention_maio_tab=pd.DataFrame({"ID":name_lista_maio, "Tweet":testo_mention_maio})

#Blocco che esegue la emotion analysis con l'ausilio della libreria preposta
emo=EmotionClassifier()
emotion_salvini=emo.predict(testo_mention_salvini)
emotion_letta=emo.predict(testo_mention_let)
emotion_meloni=emo.predict(testo_mention_mel)
emotion_maio=emo.predict(testo_mention_maio)
emotion_conte=emo.predict(testo_mention_cont)

#Blocco che esegue la sentiment analysis con l'ausilio della libreria preposta
sent=SentimentClassifier()
sentiment_salvini=sent.predict(testo_mention_salvini)
sentiment_letta=sent.predict(testo_mention_let)
sentiment_meloni=sent.predict(testo_mention_mel)
sentiment_maio=sent.predict(testo_mention_maio)
sentiment_conte=sent.predict(testo_mention_cont)

#Blocco che crea i data frame con i tweet che mensionano i singoli esponenti, l'id di coloro che lo scrivono e il risultato della sentiment ed emotion analysis
mention_sal_tab=pd.DataFrame({"ID":name_lista_salvini,"Testo":testo_mention_salvini,"Sentiment":sentiment_salvini,"Emotion":emotion_salvini, "Mention":parola_chiave_1})
mention_let_tab=pd.DataFrame({"ID":name_lista_let,"Testo":testo_mention_let,"Sentiment":sentiment_letta,"Emotion":emotion_letta,"Mention":parola_chiave_2})
mention_mel_tab=pd.DataFrame({"ID":name_lista_mel,"Testo":testo_mention_mel,"Sentiment":sentiment_meloni,"Emotion":emotion_meloni,"Mention":parola_chiave_3})
mention_conte_tab=pd.DataFrame({"ID":name_lista_cont,"Testo":testo_mention_cont,"Sentiment":sentiment_conte,"Emotion":emotion_conte,"Mention":parola_chiave_4})
mention_maio_tab=pd.DataFrame({"ID":name_lista_maio,"Testo":testo_mention_maio,"Sentiment":sentiment_maio,"Emotion":emotion_maio, "Mention":parola_chiave_5})

sentiment_emotion(mention_sal_tab,"Salvini")#Risultati della sentiment & della emotion analysis in percentuale

sentiment_emotion(mention_let_tab,"Letta")#Risultati della sentiment & della emotion analysis in percentuale

sentiment_emotion(mention_mel_tab,"Meloni")#Risultati della sentiment & della emotion analysis in percentuale

sentiment_emotion(mention_conte_tab,"Conte")#Risultati della sentiment & della emotion analysis in percentuale

sentiment_emotion(mention_maio_tab,"Di Maio")#Risultati della sentiment & della emotion analysis in percentuale

grafici_num(mention_sal_tab,"Salvini")#Numero di tweets neg, pos, neut & fel, arr, imp,tristi riferiti a Salvini

grafici_num(mention_mel_tab,"Meloni")#Numero di tweets neg, pos, neut & fel, arr, imp,tristi riferiti a Meloni

grafici_num(mention_let_tab,"Letta")#Numero di tweets neg, pos, neut & fel, arr, imp,tristi riferiti a Letta

grafici_num(mention_conte_tab,"Conte")#Numero di tweets neg, pos, neut & fel, arr, imp,tristi riferiti a Conte

grafici_num(mention_maio_tab,"Di Maio")#Numero di tweets neg, pos, neut & fel, arr, imp,tristi riferiti a Di Maio

mention_total = pd.concat([mention_sal_tab, mention_let_tab, mention_conte_tab, mention_maio_tab, mention_mel_tab]) #Unione dei 5 dataframe delle menzioni 
grafici_num(mention_total,":Total")#Risultati della sentiment & della emotion analysis in percentuale
sentiment_emotion(mention_total,":Total")#Numero di tweets neg, pos, neut & fel, arr, imp,tristi riferiti al totale

mention_total #stampa del dataframe con il totale delle mentions

grafico_social= nx.from_pandas_edgelist(mention_total, source="ID",target="Mention") #creazione di un social network graph, creando un legame tra i singoli nodi in base alla mention di riferimento
plt.figure(figsize=(20,20))#assegnazione delle dimensioni della stampa del grafico
pos = nx.spring_layout(grafico_social, k=0.80,)  #Scelta del layout con cui deve essere stampato
nx.draw(grafico_social, with_labels=True, node_color="red")
plt.show()

info=nx.info(grafico_social) #istruzione che consente di visualizzare alcune informazioni base sul network (libreria Networkx)
cliq=nx.find_cliques(grafico_social) 
cliqes=len(list(cliq)) #conteggio dei cliques
print("There are ", cliqes, "cliques into the network")
print(info)
print("Density of the Network:", nx.density(grafico_social))#stampa della densità
print("Diameter of the network:",nx.diameter(grafico_social))#stampa del diametro

#Vengono assegnati i nomi e il degree di ogni singolo nodo e poi stampate in una tabella
nome = [node for (node, val) in grafico_social.degree()]
degrees = [val for (node, val) in grafico_social.degree()]
degree_df=pd.DataFrame({"Node":nome, "Degrees":degrees})
degree_df.iloc[np.argsort(degree_df["Degrees"])]

degree_top=degree_df.loc[degree_df.Degrees.nlargest(8).index] #stampa delle informazioni precedenti su un diagramma a barre, visulizzando una top 8
degree_top.plot(x="Node", y="Degrees", kind="bar") #scelta del tipo di grafico e assegnazione agli assi delle variabili

#calcolo del nodo pià influente e creazione di un dataframe per visualizzare in ordine ascendente
degree_centrality=nxa.degree_centrality(grafico_social) 
degree_centrality_df=pd.DataFrame({"Degree_Centrality":degree_centrality})
degree_centrality_df.iloc[np.argsort(degree_centrality_df["Degree_Centrality"])]

#Funzione per il calcolo della Betweeness e messa in ordine ascendente (il miglior connettore del network)
betweeness=nx.betweenness_centrality(grafico_social)
betweeness_df=pd.DataFrame({"Betweeness":betweeness})
betweeness_df.iloc[np.argsort(betweeness_df["Betweeness"])]

#funzione per il calcolo della closeness centrality e creazione di un data frame visualizzato in ordine ascendete
closeness=nxa.closeness_centrality(grafico_social)
closeness_df=pd.DataFrame({"Closeness":closeness})
closeness_df.iloc[np.argsort(closeness_df["Closeness"])]

#funzione per il calcolo della pagerank e creazione di un data frame visualizzato in ordine ascendete
peso=nxa.pagerank(grafico_social)
peso_df=pd.DataFrame({"Pagerank":peso})
peso_df.iloc[np.argsort(peso_df["Pagerank"])]